{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Linear regression\n\nThis assignment is dedicated to Linear regression. 12 task, 30 points max.\n\nBy focusing on predicting different features of football players,\nyou will:\n* gain understanding of mathematics behind Linear Regression model\n* and become familiar with using `sklearn` library for solving this kind of tasks\n\nIn the assignment we will:\n* build Linear regression models with 1 and many variables\n* use both library methods and manual calculations using mathematical formulas\n* implement main regression metrics to evaluate performance of our models\n* explore limitations of classical linear regression\n\n### Notes:\n* do not modify `assert` blocks in code. They are used to check your results\n    \n\n### Data\n- In this notebook we will work with Football Player attributes per each match played.\n  \n  Data is taken from [European Soccer Database](https://www.kaggle.com/hugomathien/soccer) dataset. \n  \n  The table used is `Player_Attributes`.\n\n\n### Materias\n\nStart with the assignment first. Use materials below as references when needed.\n\n* NumPy:\n    - [quickstart](https://numpy.org/doc/stable/user/quickstart.html)\n    - [api reference](https://numpy.org/doc/stable/reference/index.html)\n    \n\n* Pandas:\n    - [10 minutes to pandas](https://pandas.pydata.org/docs/user_guide/10min.html)\n    - guide on [groupby.transform](https://pandas.pydata.org/docs/user_guide/groupby.html#transformation). \n      used in one of tasks on data preprocessing\n    - [cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n    \n\n* sklearn:\n    - [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) class documentation\n    - guide on [Linear models](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)\n\n\n* Matplotlib:\n    - [cheatsheet](https://matplotlib.org/cheatsheets/)\n    - examples of [main plot types](https://matplotlib.org/stable/plot_types/index.html)\n    - [scatter plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) documentation","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\npd.options.display.max_rows = 200\npd.options.display.max_columns = 100\npd.options.mode.chained_assignment = 'raise'  # forbid chained assignment to prevent implicit errors\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (7, 6)  # set default figure size\n\nimport seaborn as sns\nsns.set(font_scale=1.1)  # increase default font scale and set seaborn's plot style\n\nfrom typing import List, Iterable\nprint ('Setup complete')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:28.686367Z","start_time":"2022-02-05T15:50:27.581843Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:20.696235Z","iopub.execute_input":"2022-03-20T11:08:20.698572Z","iopub.status.idle":"2022-03-20T11:08:21.941681Z","shell.execute_reply.started":"2022-03-20T11:08:20.698216Z","shell.execute_reply":"2022-03-20T11:08:21.940828Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Data Processing","metadata":{}},{"cell_type":"code","source":"data_root_dp = os.path.join('..', 'input', 'soccer')  # change to yours path\nprint(f'data_root_dp: \"{data_root_dp}\"')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:28.701544Z","start_time":"2022-02-05T15:50:28.688368Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:21.943472Z","iopub.execute_input":"2022-03-20T11:08:21.943700Z","iopub.status.idle":"2022-03-20T11:08:21.948815Z","shell.execute_reply.started":"2022-03-20T11:08:21.943672Z","shell.execute_reply":"2022-03-20T11:08:21.948016Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### read the raw SQL data\n* you can run this code only once.<br>\n  it will read `.sqlite` file and save extracted data to a more handy `.csv` format","metadata":{}},{"cell_type":"code","source":"import sqlite3\n\n# # open connection to .sqlite file\ndata_sql_fp = os.path.join(data_root_dp, 'database.sqlite')\nprint(f'will read from: \"{data_sql_fp}\"')\ncon = sqlite3.connect(data_sql_fp)\n\ndf = pd.read_sql('select * from Player_Attributes', con)  # run SQL-query\nprint(f'shape: {df.shape}')\ncon.close()  # close connection to .sqlite file\n\n# # save to .csv\ndata_root_output = os.path.join('..', 'working')  # change to yours path\nprint(f'data_root_output: \"{data_root_output}\"')\nout_fp = os.path.join(data_root_output, 'tmp', 'player_attributes.csv')\nos.makedirs(os.path.dirname(out_fp), exist_ok=True)  # create directories if needed\nprint(f'will save to: \"{out_fp}\"')\ndf.to_csv(out_fp, index=False)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:28.762969Z","start_time":"2022-02-05T15:50:28.704803Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:21.950566Z","iopub.execute_input":"2022-03-20T11:08:21.950804Z","iopub.status.idle":"2022-03-20T11:08:32.360959Z","shell.execute_reply.started":"2022-03-20T11:08:21.950752Z","shell.execute_reply":"2022-03-20T11:08:32.360321Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### read the  data","metadata":{}},{"cell_type":"code","source":"data_root_output = os.path.join('..', 'working')  # change to yours path\ndata_player_attributes_fp = os.path.join(data_root_output,'tmp', 'player_attributes.csv')\nprint(f'reading from: \"{data_player_attributes_fp}\"')\ndf = pd.read_csv(data_player_attributes_fp)\nprint(df.shape)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.329081Z","start_time":"2022-02-05T15:50:28.764802Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:32.362325Z","iopub.execute_input":"2022-03-20T11:08:32.362771Z","iopub.status.idle":"2022-03-20T11:08:33.370760Z","shell.execute_reply.started":"2022-03-20T11:08:32.362739Z","shell.execute_reply":"2022-03-20T11:08:33.370114Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.3446Z","start_time":"2022-02-05T15:50:29.331278Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.372648Z","iopub.execute_input":"2022-03-20T11:08:33.372997Z","iopub.status.idle":"2022-03-20T11:08:33.382307Z","shell.execute_reply.started":"2022-03-20T11:08:33.372970Z","shell.execute_reply":"2022-03-20T11:08:33.381364Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T11:08:33.383499Z","iopub.execute_input":"2022-03-20T11:08:33.383799Z","iopub.status.idle":"2022-03-20T11:08:33.445774Z","shell.execute_reply.started":"2022-03-20T11:08:33.383770Z","shell.execute_reply":"2022-03-20T11:08:33.444829Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### basic data processing","metadata":{}},{"cell_type":"code","source":"# there are 3 'id' columns in data. \n# we will use 'player_api_id' only\n# and drop other 'id' columns.\nredundant_key_columns = ['id', 'player_fifa_api_id']\n\n# also we will drop categorical columns for this task.\ncategorical_columns = ['preferred_foot', 'attacking_work_rate', 'defensive_work_rate']\n\ncols_to_drop = redundant_key_columns + categorical_columns\nprint(f'will drop following columns: {cols_to_drop}')\ndf.drop(columns=cols_to_drop, inplace=True)\n\n# rename 'player_api_id' column to shorter one: 'id'.\ndf.rename(columns={'player_api_id': 'id'}, inplace=True)\n\n# cast string values to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# drop records that contain only NaNs in features\nall_features = set(df.columns).difference(['id', 'date'])  # all columns without key columns\ndf.dropna(subset=all_features, how='all', inplace=True)\n\nprint(f'df.shape: {df.shape}')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.530177Z","start_time":"2022-02-05T15:50:29.346142Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.447040Z","iopub.execute_input":"2022-03-20T11:08:33.447331Z","iopub.status.idle":"2022-03-20T11:08:33.580638Z","shell.execute_reply.started":"2022-03-20T11:08:33.447296Z","shell.execute_reply":"2022-03-20T11:08:33.579860Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# the dataset we're working with migh get updated in future (new records might be added).\n# thus we remove any records with date larger than the current max date in data (2016-07-07)\n# for compatibility with current version of notebook.\ndf.drop(index=df[df['date'] > '2016-07-07'].index, inplace=True)\nprint(df.shape)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.577558Z","start_time":"2022-02-05T15:50:29.532174Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.581554Z","iopub.execute_input":"2022-03-20T11:08:33.582018Z","iopub.status.idle":"2022-03-20T11:08:33.619914Z","shell.execute_reply.started":"2022-03-20T11:08:33.581976Z","shell.execute_reply":"2022-03-20T11:08:33.618860Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### drop duplicates\n\n* we should **ALWAYS** understand what our data represents\n\n\n* here we are dealing with Football Player attributes per each match played.\n\n  so we expect the data to have `(id, date)` as a key\n  \n  \n* let's check whether our assumption holds","metadata":{"ExecuteTime":{"end_time":"2022-01-28T12:35:41.941544Z","start_time":"2022-01-28T12:35:41.883366Z"}}},{"cell_type":"code","source":"df.duplicated(['id', 'date']).value_counts()","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.608747Z","start_time":"2022-02-05T15:50:29.580864Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.621113Z","iopub.execute_input":"2022-03-20T11:08:33.621372Z","iopub.status.idle":"2022-03-20T11:08:33.643859Z","shell.execute_reply.started":"2022-03-20T11:08:33.621341Z","shell.execute_reply":"2022-03-20T11:08:33.642890Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"* there are no duplicates if considering `(id, date)` pair as a key","metadata":{}},{"cell_type":"code","source":"df.duplicated('id').value_counts()","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.624155Z","start_time":"2022-02-05T15:50:29.609731Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.645441Z","iopub.execute_input":"2022-03-20T11:08:33.646326Z","iopub.status.idle":"2022-03-20T11:08:33.660292Z","shell.execute_reply.started":"2022-03-20T11:08:33.646281Z","shell.execute_reply":"2022-03-20T11:08:33.659491Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"* and there are duplicates if considering only `id` column as a key","metadata":{}},{"cell_type":"markdown","source":"* so our assumption holds. but let's work only with latest stats for each player","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Task 1. Keep only latest data for each player (1 point)\n\n* HINT: you can use \n\n  `df[<new_column>] = df.groupby(<key_to_aggregate>)[<column>].transform('max')` \n  \n  to get the `max` value of `<column>` for each group specified by `<key_to_aggregate>` columns.\n  \n  then you'll have to filter rows using `<column>` and `<new_column>` values\n  \n\n* don't forget to remove any additional columns you introduce here","metadata":{}},{"cell_type":"code","source":"# # your code here\n\ndf['last_date'] = df.groupby(['id'])['date'].transform('max')\ndf.drop(columns= ['date'], inplace = True)\ndf.drop_duplicates(subset = ['id'], inplace=True)\n\n\nprint(df.shape)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.639771Z","start_time":"2022-02-05T15:50:29.625866Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.661632Z","iopub.execute_input":"2022-03-20T11:08:33.662309Z","iopub.status.idle":"2022-03-20T11:08:33.716268Z","shell.execute_reply.started":"2022-03-20T11:08:33.662246Z","shell.execute_reply":"2022-03-20T11:08:33.715391Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"assert df.duplicated(['id']).sum() == 0\nassert df.shape == (11060, 37)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.733313Z","start_time":"2022-02-05T15:50:29.718598Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.717496Z","iopub.execute_input":"2022-03-20T11:08:33.717805Z","iopub.status.idle":"2022-03-20T11:08:33.726214Z","shell.execute_reply.started":"2022-03-20T11:08:33.717769Z","shell.execute_reply":"2022-03-20T11:08:33.725335Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# no longer need 'id' and 'date' columns\ndf.drop(columns=['id'], inplace=True)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.748115Z","start_time":"2022-02-05T15:50:29.735521Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.728450Z","iopub.execute_input":"2022-03-20T11:08:33.729258Z","iopub.status.idle":"2022-03-20T11:08:33.737859Z","shell.execute_reply.started":"2022-03-20T11:08:33.729214Z","shell.execute_reply":"2022-03-20T11:08:33.737231Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NaNs","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.763295Z","start_time":"2022-02-05T15:50:29.750116Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.741085Z","iopub.execute_input":"2022-03-20T11:08:33.741813Z","iopub.status.idle":"2022-03-20T11:08:33.755083Z","shell.execute_reply.started":"2022-03-20T11:08:33.741768Z","shell.execute_reply":"2022-03-20T11:08:33.754432Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"* do you see any pattern in how NaNs are distributed?","metadata":{}},{"cell_type":"code","source":"df.isna().sum(axis=1).value_counts()","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.77865Z","start_time":"2022-02-05T15:50:29.765289Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.758014Z","iopub.execute_input":"2022-03-20T11:08:33.758424Z","iopub.status.idle":"2022-03-20T11:08:33.768606Z","shell.execute_reply.started":"2022-03-20T11:08:33.758394Z","shell.execute_reply":"2022-03-20T11:08:33.767508Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Task 2. Drop rows that contain NaNs (1 point)","metadata":{}},{"cell_type":"code","source":"# your code here\ncolumns_with_nan = ['overall_rating', 'potential', 'crossing', 'finishing',\n       'heading_accuracy', 'short_passing', 'volleys', 'dribbling', 'curve',\n       'free_kick_accuracy', 'long_passing', 'ball_control', 'acceleration',\n       'sprint_speed', 'agility', 'reactions', 'balance', 'shot_power',\n       'jumping', 'stamina', 'strength', 'long_shots', 'aggression',\n       'interceptions', 'positioning', 'vision', 'penalties', 'marking',\n       'standing_tackle', 'sliding_tackle', 'gk_diving', 'gk_handling',\n       'gk_kicking', 'gk_positioning', 'gk_reflexes', 'last_date']\ndf.dropna(subset=columns_with_nan, inplace=True)\n","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.809854Z","start_time":"2022-02-05T15:50:29.796711Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.769865Z","iopub.execute_input":"2022-03-20T11:08:33.770394Z","iopub.status.idle":"2022-03-20T11:08:33.783460Z","shell.execute_reply.started":"2022-03-20T11:08:33.770352Z","shell.execute_reply":"2022-03-20T11:08:33.782558Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"assert df.isna().sum().sum() == 0","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.840082Z","start_time":"2022-02-05T15:50:29.82562Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.784611Z","iopub.execute_input":"2022-03-20T11:08:33.784837Z","iopub.status.idle":"2022-03-20T11:08:33.800662Z","shell.execute_reply.started":"2022-03-20T11:08:33.784810Z","shell.execute_reply":"2022-03-20T11:08:33.799832Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 3. Implement main regression metrics yourself (up to 4 points):\n* Mean Squared Error (mse) - **(1 point)**\n* Mean Absolute Error (mae) - **(1 point)**\n* Mean Absolute Percentage Error (mape) - **(1 point)**\n* R-squared (r2) - **(1 point)**\n\n\n#### bonus point: MAPE is implemented in such way to take possible 0 in `y_true` into account\nHints:\n\n* you should divide on \n  $ \\hat{y}_{true} = (\\hat{y}_{true}^{(0)}, \\hat{y}_{true}^{(1)}, ..., \\hat{y}_{true}^{(n)}) $,\n  \n  where $ \\ \\ \\hat{y}_{true}^{(i)} = max(\\ \\ | y_{true}^{(i)} |, \\ \\ \\epsilon), \\ \\ \\epsilon > 0, \\ \\ y_{true}^{(i)} \\in \\mathbb R $\n  \n\n* for compatibility with `sklearn` take \n\n  `eps = np.finfo(np.float64).eps`\n  \n  \n#### bonus point: $R^2$ is implemented in such way to take possible 0 in numerator and denominator into account\nHints:\n\n* recall the formula: $ R^2 = 1 - \\dfrac{SS_{residuals}}{SS_{total}} $\n\n\n* you should return:\n\n    $\n    \\begin{cases}\n     R^2 & \\text{ if } SS_{residuals} \\ne 0 \\text{ and  } SS_{total} \\ne 0 \n     \\\\\n     0 & \\text{ if } SS_{residuals} \\ne 0 \\text{ and  } SS_{total} = 0 \n     \\\\\n     1 & \\text { otherwise }\n    \\end{cases}\n    $\n    \n\n* for a better understanding of $R^2$ coefficient \n  try to explain to yourself why we return those values in each of edge cases","metadata":{}},{"cell_type":"code","source":"# we use '*' in the beggining of each function signature\n# to forbid positional arguments and enforce the use of kwargs.\n# this helps to avoid mistake when arguments are passed in wrong order.\n\n# write down code instead of raising NotImplementedError in each of functions below.\n\ndef mse(*, y_true, y_pred):\n    mse_value = (np.square(y_true - y_pred)).mean(axis=0)\n    return mse_value\n    #raise NotImplementedError()\n    \ndef mae(*, y_true, y_pred):\n    mae_value = (np.absolute(y_true - y_pred)).mean(axis=0)\n    return mae_value\n#    raise NotImplementedError()\n    \ndef mape(*, y_true, y_pred):\n    eps = np.finfo(np.float64).eps\n    y_true_max = np.absolute(y_true.copy())\n    y_true_max[y_true_max<eps] = eps\n    mape_value = (np.absolute(y_true - y_pred)/np.absolute(y_true_max)).mean(axis=0)\n    return mape_value\n#    raise NotImplementedError()\n    \ndef r2(*, y_true, y_pred):\n    SSresiduals = (np.square(y_true-y_pred)).mean(axis = 0)\n    SStotal = (np.square(y_true-y_true.mean())).mean(axis=0)\n    if SStotal != 0 and SSresiduals != 0:\n        r_squared =  1- SSresiduals / SStotal\n        return r_squared\n    elif SStotal == 0 and SSresiduals != 0:\n        return 0\n    return 1\n#    raise NotImplementedError()","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.85563Z","start_time":"2022-02-05T15:50:29.842173Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.801891Z","iopub.execute_input":"2022-03-20T11:08:33.802153Z","iopub.status.idle":"2022-03-20T11:08:33.812797Z","shell.execute_reply.started":"2022-03-20T11:08:33.802072Z","shell.execute_reply":"2022-03-20T11:08:33.811881Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validate your implementations are correct\n\n* we'll use synthetic data for this task\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.94853Z","start_time":"2022-02-05T15:50:29.873343Z"},"execution":{"iopub.status.busy":"2022-03-18T08:05:31.753178Z","iopub.execute_input":"2022-03-18T08:05:31.753599Z","iopub.status.idle":"2022-03-18T08:05:31.963264Z","shell.execute_reply.started":"2022-03-18T08:05:31.753566Z","shell.execute_reply":"2022-03-18T08:05:31.962361Z"}}},{"cell_type":"code","source":"from sklearn.metrics import (\nmean_squared_error as mse_lib, \nmean_absolute_error as mae_lib,\nmean_absolute_percentage_error as mape_lib,\nr2_score as r2_lib\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T11:08:33.813970Z","iopub.execute_input":"2022-03-20T11:08:33.814205Z","iopub.status.idle":"2022-03-20T11:08:33.994086Z","shell.execute_reply.started":"2022-03-20T11:08:33.814153Z","shell.execute_reply":"2022-03-20T11:08:33.993325Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def compare_metrics(fn, fn_lib, y_true, y_pred, n_digits=9):\n    \"\"\" Print and compare values of custom and library functions using given precision. \"\"\"\n    m_fn = fn(y_true=y_true, y_pred=y_pred)\n    m_fn_lib = fn_lib(y_true=y_true, y_pred=y_pred)\n    print(f'{fn.__name__} custom : {m_fn}')\n    print(f'{fn.__name__} library: {m_fn_lib}')\n    print()\n    assert round(m_fn, n_digits) == round(m_fn_lib, n_digits)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:29.964603Z","start_time":"2022-02-05T15:50:29.95253Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:33.995532Z","iopub.execute_input":"2022-03-20T11:08:33.995749Z","iopub.status.idle":"2022-03-20T11:08:34.001890Z","shell.execute_reply.started":"2022-03-20T11:08:33.995722Z","shell.execute_reply":"2022-03-20T11:08:34.001078Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### a case with a good fit","metadata":{}},{"cell_type":"code","source":"n = 30\nx = np.arange(n)\ny_true = 2 * x + 3 + np.random.normal(loc=0, scale=2, size=n)\ny_pred = 2 * x + 3\n\nplt.plot(x, y_true, label='y_true')\nplt.plot(x, y_pred, label='y_pred')\nplt.legend();","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.136424Z","start_time":"2022-02-05T15:50:29.965777Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:34.003141Z","iopub.execute_input":"2022-03-20T11:08:34.003402Z","iopub.status.idle":"2022-03-20T11:08:34.356917Z","shell.execute_reply.started":"2022-03-20T11:08:34.003366Z","shell.execute_reply":"2022-03-20T11:08:34.356044Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"compare_metrics(mse, mse_lib, y_true=y_true, y_pred=y_pred)\ncompare_metrics(mae, mae_lib, y_true=y_true, y_pred=y_pred)\ncompare_metrics(mape, mape_lib, y_true=y_true, y_pred=y_pred)\ncompare_metrics(r2, r2_lib, y_true=y_true, y_pred=y_pred)\nprint('all tests passed')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.152383Z","start_time":"2022-02-05T15:50:30.138337Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:34.358134Z","iopub.execute_input":"2022-03-20T11:08:34.358387Z","iopub.status.idle":"2022-03-20T11:08:34.369030Z","shell.execute_reply.started":"2022-03-20T11:08:34.358359Z","shell.execute_reply":"2022-03-20T11:08:34.368211Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 0s in target. MAPE should explode","metadata":{}},{"cell_type":"code","source":"n = 30\nx = np.arange(n)\ny_true = 2 * x + 3 + np.random.normal(loc=0, scale=2, size=n)\ny_true[:7] = 0\ny_pred = 2 * x + 3\n\nplt.plot(x, y_true, label='y_true')\nplt.plot(x, y_pred, label='y_pred')\nplt.legend();","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.341643Z","start_time":"2022-02-05T15:50:30.154274Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:34.370423Z","iopub.execute_input":"2022-03-20T11:08:34.370738Z","iopub.status.idle":"2022-03-20T11:08:34.665549Z","shell.execute_reply.started":"2022-03-20T11:08:34.370694Z","shell.execute_reply":"2022-03-20T11:08:34.664734Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"compare_metrics(mse, mse_lib, y_true=y_true, y_pred=y_pred)\ncompare_metrics(mae, mae_lib, y_true=y_true, y_pred=y_pred)\ncompare_metrics(mape, mape_lib, y_true=y_true, y_pred=y_pred)\ncompare_metrics(r2, r2_lib, y_true=y_true, y_pred=y_pred)\nprint('all tests passed')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.357566Z","start_time":"2022-02-05T15:50:30.343441Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:34.666745Z","iopub.execute_input":"2022-03-20T11:08:34.666962Z","iopub.status.idle":"2022-03-20T11:08:34.677142Z","shell.execute_reply.started":"2022-03-20T11:08:34.666935Z","shell.execute_reply":"2022-03-20T11:08:34.675946Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### edge cases for R2","metadata":{}},{"cell_type":"code","source":"# constant target value\n\nn = 30\nx = np.arange(n)\ny_true = np.repeat(15, n)\ny_pred = 2 * x + 3\n\ncompare_metrics(r2, r2_lib, y_true=y_true, y_pred=y_pred)\nprint('success')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.372953Z","start_time":"2022-02-05T15:50:30.359888Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:34.678618Z","iopub.execute_input":"2022-03-20T11:08:34.679384Z","iopub.status.idle":"2022-03-20T11:08:34.689820Z","shell.execute_reply.started":"2022-03-20T11:08:34.679333Z","shell.execute_reply":"2022-03-20T11:08:34.688845Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# perfect fit\n\nn = 30\nx = np.arange(n)\ny_true = 2 * x + 3 + np.random.normal(loc=0, scale=2, size=n)\ny_pred = y_true\n\ncompare_metrics(r2, r2_lib, y_true=y_true, y_pred=y_pred)\nprint('success')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.388956Z","start_time":"2022-02-05T15:50:30.374956Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:34.691118Z","iopub.execute_input":"2022-03-20T11:08:34.691543Z","iopub.status.idle":"2022-03-20T11:08:34.702264Z","shell.execute_reply.started":"2022-03-20T11:08:34.691495Z","shell.execute_reply":"2022-03-20T11:08:34.701421Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define helper functions","metadata":{}},{"cell_type":"code","source":"def get_metrics(*, y_true, y_pred) -> pd.Series:\n    \"\"\" A helper function to return all metrics for given labels and predictions. \"\"\"\n    \n    # if you didn't implement some metrics, comment them out for this function to work.\n    return pd.Series({\n        'mse': mse(y_true=y_true, y_pred=y_pred),\n        'mae': mae(y_true=y_true, y_pred=y_pred),\n        'mape': mape(y_true=y_true, y_pred=y_pred),\n        'r2': r2(y_true=y_true, y_pred=y_pred),\n    })","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.404958Z","start_time":"2022-02-05T15:50:30.389956Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:34.703929Z","iopub.execute_input":"2022-03-20T11:08:34.704143Z","iopub.status.idle":"2022-03-20T11:08:34.710262Z","shell.execute_reply.started":"2022-03-20T11:08:34.704117Z","shell.execute_reply":"2022-03-20T11:08:34.709633Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def fit_and_evaluate(model, x_train, y_train, x_test, y_test) -> pd.DataFrame:\n    \"\"\" \n    A helper function to: \n    * fit a passed model\n    * and return metrics on train and test sets.\n    \"\"\"\n    # fit the model\n    model.fit(x_train, y_train)\n    \n    # return metrics as pd.DataFrame\n    m = pd.DataFrame([\n        get_metrics(y_true=y_train, y_pred=model.predict(x_train)),  # metrics for train set\n        get_metrics(y_true=y_test, y_pred=model.predict(x_test))     # metrics for test set\n    ], index=['train', 'test']\n    )\n    \n    return m","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.420955Z","start_time":"2022-02-05T15:50:30.406959Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:34.711410Z","iopub.execute_input":"2022-03-20T11:08:34.711993Z","iopub.status.idle":"2022-03-20T11:08:34.725093Z","shell.execute_reply.started":"2022-03-20T11:08:34.711962Z","shell.execute_reply":"2022-03-20T11:08:34.724234Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train/test split\n\n* Split the data we have into Train and Test splits with 80%/20% proportion","metadata":{}},{"cell_type":"code","source":"print(df.shape)\nix_split = int(0.8 * df.shape[0])\nDF_TRAIN = df.iloc[:ix_split].copy()\nDF_TEST = df.iloc[ix_split:].copy()\nprint(DF_TRAIN.shape, DF_TEST.shape)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.436855Z","start_time":"2022-02-05T15:50:30.422956Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:34.726471Z","iopub.execute_input":"2022-03-20T11:08:34.726943Z","iopub.status.idle":"2022-03-20T11:08:34.739317Z","shell.execute_reply.started":"2022-03-20T11:08:34.726912Z","shell.execute_reply":"2022-03-20T11:08:34.738378Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def get_train_test_data(features: List[str], target: str) -> tuple:\n    \"\"\"\n    Return x_train, y_train, x_test, y_test using \n    * global train/test split \n    * features and target provided.\n    \"\"\"\n    # in general, it's a bad practice to use global variables. \n    # however, we use it here with caution for simplicity.\n    return (\n        DF_TRAIN[features].copy(), DF_TRAIN[target].copy(), \n        DF_TEST[features].copy(), DF_TEST[target].copy()\n    )","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.452547Z","start_time":"2022-02-05T15:50:30.438523Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:34.740851Z","iopub.execute_input":"2022-03-20T11:08:34.741155Z","iopub.status.idle":"2022-03-20T11:08:34.748989Z","shell.execute_reply.started":"2022-03-20T11:08:34.741115Z","shell.execute_reply":"2022-03-20T11:08:34.748409Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear regression with 1 variable\n\n* Let's start with Linear Regression that predicts target variable \n  using only 1 predictor (independent variable)  \n\n* We will try to predict `dribbling` variable using `short_passing` as a predictor","metadata":{}},{"cell_type":"code","source":"target = 'dribbling'\nfeatures = ['short_passing']\nx_train, y_train, x_test, y_test = get_train_test_data(features, target)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.468403Z","start_time":"2022-02-05T15:50:30.454412Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:34.749947Z","iopub.execute_input":"2022-03-20T11:08:34.750483Z","iopub.status.idle":"2022-03-20T11:08:34.765942Z","shell.execute_reply.started":"2022-03-20T11:08:34.750447Z","shell.execute_reply":"2022-03-20T11:08:34.765232Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's visualize our data first","metadata":{}},{"cell_type":"code","source":"plt.scatter(x=x_train, y=y_train, alpha=0.3, s=10);\nplt.xlabel('short_passing');\nplt.ylabel('dribbling');","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.62576Z","start_time":"2022-02-05T15:50:30.472404Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:34.767036Z","iopub.execute_input":"2022-03-20T11:08:34.767515Z","iopub.status.idle":"2022-03-20T11:08:35.068797Z","shell.execute_reply.started":"2022-03-20T11:08:34.767478Z","shell.execute_reply":"2022-03-20T11:08:35.068137Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"* do you notice anything interesting on this plot?\n* if so, how can we use it later to generate more accurate predictions?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Task 4. Compute Pearson correlation coefficient between feature and target (1 pont)\nHint: you can use either `pd.Series.corr` method or `np.corrcoef` function","metadata":{}},{"cell_type":"code","source":"# your code here\n\ncorrelation = np.corrcoef(df['short_passing'], df['dribbling'])\ncorrelation","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.641483Z","start_time":"2022-02-05T15:50:30.627267Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.069945Z","iopub.execute_input":"2022-03-20T11:08:35.070317Z","iopub.status.idle":"2022-03-20T11:08:35.078464Z","shell.execute_reply.started":"2022-03-20T11:08:35.070278Z","shell.execute_reply":"2022-03-20T11:08:35.077782Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Solve using `sklearn`\n\n* we will use `LinearRegression` class from `sklearn` library \n  to fit a linear regression model and use it to generate prediction","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.750756Z","start_time":"2022-02-05T15:50:30.704711Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.080242Z","iopub.execute_input":"2022-03-20T11:08:35.081028Z","iopub.status.idle":"2022-03-20T11:08:35.174021Z","shell.execute_reply.started":"2022-03-20T11:08:35.080983Z","shell.execute_reply":"2022-03-20T11:08:35.173362Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"model = LinearRegression(fit_intercept=True)\nmodel.fit(x_train, y_train);","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.765995Z","start_time":"2022-02-05T15:50:30.752757Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.175387Z","iopub.execute_input":"2022-03-20T11:08:35.176329Z","iopub.status.idle":"2022-03-20T11:08:35.196123Z","shell.execute_reply.started":"2022-03-20T11:08:35.176281Z","shell.execute_reply":"2022-03-20T11:08:35.195449Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"* explore fitted model parameters","metadata":{}},{"cell_type":"code","source":"model_intercept = model.intercept_\nmodel_slope = model.coef_[0]\nprint(f'model_intercept: {model_intercept}')\nprint(f'model_slope: {model_slope}')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.781716Z","start_time":"2022-02-05T15:50:30.768504Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.197823Z","iopub.execute_input":"2022-03-20T11:08:35.198444Z","iopub.status.idle":"2022-03-20T11:08:35.204868Z","shell.execute_reply.started":"2022-03-20T11:08:35.198400Z","shell.execute_reply":"2022-03-20T11:08:35.203899Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"* use fitted model to get predictions for test set","metadata":{}},{"cell_type":"code","source":"model_preds = model.predict(x_test)\nmodel_preds","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.79686Z","start_time":"2022-02-05T15:50:30.783228Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.206360Z","iopub.execute_input":"2022-03-20T11:08:35.206651Z","iopub.status.idle":"2022-03-20T11:08:35.219759Z","shell.execute_reply.started":"2022-03-20T11:08:35.206612Z","shell.execute_reply":"2022-03-20T11:08:35.218747Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"* explore model quality on test set.","metadata":{}},{"cell_type":"code","source":"print('model metrics on test set')\nmetrics_test_1 = get_metrics(y_true=y_test, y_pred=model_preds)\nmetrics_test_1","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:30.859941Z","start_time":"2022-02-05T15:50:30.849075Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.221242Z","iopub.execute_input":"2022-03-20T11:08:35.221693Z","iopub.status.idle":"2022-03-20T11:08:35.238211Z","shell.execute_reply.started":"2022-03-20T11:08:35.221629Z","shell.execute_reply":"2022-03-20T11:08:35.237247Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"* can you explain what those metic values mean?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Task 5. Calculate predictions for test set manually (1 point)\n\nAbove we used `LinearRegression.predict()` method to obtain predictions. \nNow let's do it manually!\n\nRecall Linear Regression formula:\n\n* $ \\hat{y} = w_0 + w_1 \\cdot x, $ \n\n    where:\n    * $w_0$ is an intercept\n    * $w_1$ is a slope\n    * $x$ is a predictor\n    * $\\hat{y}$ is a predicted variable\n  \n\n* use intercept and slope values from fitted LinearRegression model","metadata":{}},{"cell_type":"code","source":"# use `x_test` for calculations as is (it's a DataFrame with 1 column).\n# do not convert it to Series or numpy array.\n\n# your code here:\nmodel_manual_preds = model_intercept+model_slope*x_test","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:31.109063Z","start_time":"2022-02-05T15:50:31.097327Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.239917Z","iopub.execute_input":"2022-03-20T11:08:35.240227Z","iopub.status.idle":"2022-03-20T11:08:35.245496Z","shell.execute_reply.started":"2022-03-20T11:08:35.240185Z","shell.execute_reply":"2022-03-20T11:08:35.244459Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"assert isinstance(model_manual_preds, pd.DataFrame)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:31.169787Z","start_time":"2022-02-05T15:50:31.156073Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.247044Z","iopub.execute_input":"2022-03-20T11:08:35.247541Z","iopub.status.idle":"2022-03-20T11:08:35.255739Z","shell.execute_reply.started":"2022-03-20T11:08:35.247498Z","shell.execute_reply":"2022-03-20T11:08:35.255015Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# ensure our manual predictions are the same as model generated\nassert np.allclose(model_preds, model_manual_preds.iloc[:, 0].to_numpy())\nprint('success')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:31.185196Z","start_time":"2022-02-05T15:50:31.171743Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.263082Z","iopub.execute_input":"2022-03-20T11:08:35.263829Z","iopub.status.idle":"2022-03-20T11:08:35.270598Z","shell.execute_reply.started":"2022-03-20T11:08:35.263781Z","shell.execute_reply":"2022-03-20T11:08:35.269643Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Task 6. Plot test data and predicted values (1 point)\n\n* use `plt.scatter` function as in example above.\n* first build scatter plot for test data\n* then build scatter plot with model predictions\n* add plot title, axis names and legend\n\nHints:\n* use `alpha` parameter to control opacity and `s` parameter to control size of points.\n  this will make plot more readable\n* use `label` parameter to add label to each set of points added to scatter plot\n* use different color for model predictions. control with parameter `color`","metadata":{}},{"cell_type":"code","source":"# # your code here\nplt.title('Plot test data and predicted values')\nplt.scatter(x=x_test, y=y_test, alpha=0.3, s=10, color = 'blue' );\n\nplt.scatter(x=x_test, y=model_manual_preds, alpha=0.3, s=15, color = 'red');\nplt.legend(['test','model'])\nplt.xlabel('short_passing');\nplt.ylabel('dribbing');","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:31.248662Z","start_time":"2022-02-05T15:50:31.242488Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.271608Z","iopub.execute_input":"2022-03-20T11:08:35.272149Z","iopub.status.idle":"2022-03-20T11:08:35.671023Z","shell.execute_reply.started":"2022-03-20T11:08:35.272115Z","shell.execute_reply":"2022-03-20T11:08:35.670209Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Task 7.  Fit regression model manually (3 points)","metadata":{"ExecuteTime":{"end_time":"2022-02-02T18:04:08.699161Z","start_time":"2022-02-02T18:04:08.686331Z"}}},{"cell_type":"markdown","source":"So far we have used `sklearn` library to fit linear regression model and obtain predictions. Good job!\n\nRecall the formula of Linear regression for 1 variable:\n\n* $ \\hat{y} = w_0 + w_1 \\cdot x $\n\nNow let's try to fit the model **ourselves**.","metadata":{"ExecuteTime":{"end_time":"2022-02-02T18:04:08.699161Z","start_time":"2022-02-02T18:04:08.686331Z"}}},{"cell_type":"markdown","source":"### Theory\n\nAssume our dataset consists of _n_ samples.\n\nTo fit a Linear regression model means to find the best set of parameters - \n$(w_0, w_1)$ that minimizes a **Loss function**.<br>\nIn case of Linear regression we choose **Residual sum of squares (RSS)** as a Loss function:\n\n* $ L(w, x, y) = RSS(w, x, y) = \\sum_{i=0}^{n-1} (y_i - \\hat{y}_i ) ^ 2 \\rightarrow min $\n\nThat is conceptually the same as minimizing Mean squared error (MSE):\n\n* $ MSE(w, x, y) = \\dfrac{1}{n} \\sum_{i=0}^{n-1} (y_i - \\hat{y}_i ) ^ 2 \\rightarrow min $\n\nHere:\n* $y = (y_0, y_2, ..., y_{n-1}) $ - is a vector of target values we want to predict.\n  1 value per each sample in our dataset that consists of _n_ samples\n* $\\hat{y} = (\\hat{y}_0, \\hat{y}_2, ..., \\hat{y}_{n-1}) $ - is a vector of predictions \n* $\\hat{y}_i = w_0 + w_1 \\cdot x_i$ - is an individual prediction for _i_-th sample in a dataset\n\n---\n\nBest parameters $w_0$ and $w_1$ can be found using **following formulas**:\n\n* $w_0 = \\bar{y} - w_1 \\cdot \\bar{x}$\n\n\n* $w_1 = \\dfrac{\\sum_{i=0}^{n-1}(x_i - \\bar{x}) y_i}{\\sum_{i=0}^{n-1}(x_i - \\bar{x})^2}$\n\n\n* where:\n * $\\bar{x} = \\dfrac{1}{n} \\sum_{i=0}^{n-1}x_i$ - is the mean of vector $x$\n \n * $\\bar{y} = \\dfrac{1}{n} \\sum_{i=0}^{n-1}y_i$ - is the mean of vector $y$\n \nYou should use these formulas to find the best model parameters in this task.\n \n---\n\nIf you want to learn more you can refer to \n[Simple linear regression](https://en.wikipedia.org/wiki/Simple_linear_regression) wiki-page.\n\nIt contains analagous formula for $w_1$ coefficient that is conceptually the same \n(so it's not a bug in our formula provided above).","metadata":{"ExecuteTime":{"end_time":"2022-02-02T18:04:08.699161Z","start_time":"2022-02-02T18:04:08.686331Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Implementation","metadata":{}},{"cell_type":"markdown","source":"* convert pd.DataFrame and pd.Series (`x_train` and `y_train` respecitvely) to numpy-arrays","metadata":{}},{"cell_type":"code","source":"x = x_train.iloc[:, 0].to_numpy()\ny = y_train.to_numpy()","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:31.870582Z","start_time":"2022-02-05T15:50:31.860585Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.672298Z","iopub.execute_input":"2022-03-20T11:08:35.672980Z","iopub.status.idle":"2022-03-20T11:08:35.677784Z","shell.execute_reply.started":"2022-03-20T11:08:35.672928Z","shell.execute_reply":"2022-03-20T11:08:35.676964Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"* calculate model parameters ($w_1$ - slope and $w_0$ - intercept) using formulas given above","metadata":{}},{"cell_type":"code","source":"# # your code here:\nmanual_slope = np.sum((x-x.mean(axis=0))*y)/np.sum(np.square(x-x.mean(axis=0)))\nmanual_intercept = y.mean(axis = 0)-manual_slope*x.mean(axis= 0)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:31.980524Z","start_time":"2022-02-05T15:50:31.972377Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.679095Z","iopub.execute_input":"2022-03-20T11:08:35.679364Z","iopub.status.idle":"2022-03-20T11:08:35.690924Z","shell.execute_reply.started":"2022-03-20T11:08:35.679332Z","shell.execute_reply":"2022-03-20T11:08:35.690285Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"print(f'manual_intercept: {manual_intercept}')\nprint(f'manual_slope: {manual_slope}')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:32.011182Z","start_time":"2022-02-05T15:50:31.997992Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.692169Z","iopub.execute_input":"2022-03-20T11:08:35.692543Z","iopub.status.idle":"2022-03-20T11:08:35.703727Z","shell.execute_reply.started":"2022-03-20T11:08:35.692513Z","shell.execute_reply":"2022-03-20T11:08:35.702854Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"assert round(model_intercept, 9) == round(manual_intercept, 9)\nassert round(model_slope, 9) == round(manual_slope, 9)\nprint('success')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:32.026832Z","start_time":"2022-02-05T15:50:32.015585Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.704733Z","iopub.execute_input":"2022-03-20T11:08:35.705133Z","iopub.status.idle":"2022-03-20T11:08:35.715388Z","shell.execute_reply.started":"2022-03-20T11:08:35.705101Z","shell.execute_reply":"2022-03-20T11:08:35.714500Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear regression with multiple variables\n\n* Now let's use Linear Regression model that uses multiple variables\n\n* We will try to predict `dribbling` variable using \n  `['ball_control', 'short_passing', 'strength', 'sprint_speed']` features","metadata":{}},{"cell_type":"code","source":"target = 'dribbling'\nfeatures = ['ball_control', 'short_passing', 'strength', 'sprint_speed']\nx_train, y_train, x_test, y_test = get_train_test_data(features, target)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:32.152631Z","start_time":"2022-02-05T15:50:32.128476Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.716659Z","iopub.execute_input":"2022-03-20T11:08:35.717046Z","iopub.status.idle":"2022-03-20T11:08:35.728557Z","shell.execute_reply.started":"2022-03-20T11:08:35.717002Z","shell.execute_reply":"2022-03-20T11:08:35.727632Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's explore the data first","metadata":{}},{"cell_type":"code","source":"cols = features + [target]\nprint(cols)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:32.261634Z","start_time":"2022-02-05T15:50:32.255334Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.729929Z","iopub.execute_input":"2022-03-20T11:08:35.730339Z","iopub.status.idle":"2022-03-20T11:08:35.738775Z","shell.execute_reply.started":"2022-03-20T11:08:35.730291Z","shell.execute_reply":"2022-03-20T11:08:35.738216Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Task 8 (1 point)\n\nBuild pairwise correlation matrix for selected features and target. Visualize it with heatmap.\n\nHints:\n* use `DF_TRAIN` to build statistics (`x_train` contains only features. target is in `y_train`)\n* use `pd.DataFrame.corr()` to build correlation matrix\n* use `sns.heatmap` to build heatmap from correlation matrix. \n  * use `center` parameter to center your heatmap's colormap at 0\n  * use diverging colormap (`cmap` parameter)<br>\n    you can find the list of diverging colormaps\n    [here](https://matplotlib.org/stable/tutorials/colors/colormaps.html#diverging)<br>\n    the reason is that we need to easily find both negative and positive correlations.\n  * use `annot` parameters to add numbers to the plot.\n* you can print `pd.DataFrame` and build plot in the same cell.<br> \n  use `display(...)` function that accets `pd.DataFrame` before building the plot\n  \nYou can refer to [seaborn.heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) documentation","metadata":{}},{"cell_type":"code","source":"# # your code here\ncorrelation_matrix = DF_TRAIN[cols].corr()\ndisplay(correlation_matrix)\nsns.heatmap( correlation_matrix,center = 0, annot=True, cmap = 'BuPu');","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:32.387597Z","start_time":"2022-02-05T15:50:32.373491Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:35.739825Z","iopub.execute_input":"2022-03-20T11:08:35.740354Z","iopub.status.idle":"2022-03-20T11:08:36.134806Z","shell.execute_reply.started":"2022-03-20T11:08:35.740323Z","shell.execute_reply":"2022-03-20T11:08:36.133994Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"* what can you say about selected feature set using this correlation matrix?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's build pairplots for features and target\n\n* `plot_kws` parameter accepts dictionary of keyword arguments \n  that are passed to bivariate plotting function (`sns.scatterplot` in this case)\n\n\n* we set size to 7 (using `s` parameter) to make points smaller and see patterns clearly.<br>\n  you can compare this to running with not `plot_kws` passed.\n  \n\n* `alpha` parameter also helps to see data distribution clearly\n\n\n* you can also experiment with `kind` parameter - refer to\n  [seaborn.pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html)\n  documentation for possible values.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(DF_TRAIN[cols], plot_kws={'s': 7, 'alpha': 0.5});","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:41.364056Z","start_time":"2022-02-05T15:50:32.950842Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:36.136039Z","iopub.execute_input":"2022-03-20T11:08:36.136288Z","iopub.status.idle":"2022-03-20T11:08:42.074267Z","shell.execute_reply.started":"2022-03-20T11:08:36.136260Z","shell.execute_reply":"2022-03-20T11:08:42.073306Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"* what can you say about these scatterplots? is there anything we can use later?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### Solve using `sklearn`","metadata":{}},{"cell_type":"code","source":"model = LinearRegression(fit_intercept=True)\nmodel.fit(x_train, y_train);","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:41.394842Z","start_time":"2022-02-05T15:50:41.368053Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.075599Z","iopub.execute_input":"2022-03-20T11:08:42.076235Z","iopub.status.idle":"2022-03-20T11:08:42.089227Z","shell.execute_reply.started":"2022-03-20T11:08:42.076186Z","shell.execute_reply":"2022-03-20T11:08:42.088445Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* explore fitted model parameters","metadata":{}},{"cell_type":"code","source":"model_intercept = model.intercept_\nmodel_coef = model.coef_\nprint(f'model_intercept: {model_intercept}')\nprint(f'model_coef: {model_coef}')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:41.410999Z","start_time":"2022-02-05T15:50:41.398344Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.090521Z","iopub.execute_input":"2022-03-20T11:08:42.091211Z","iopub.status.idle":"2022-03-20T11:08:42.099463Z","shell.execute_reply.started":"2022-03-20T11:08:42.091159Z","shell.execute_reply":"2022-03-20T11:08:42.098267Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"* how to interpret these model parameters?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* use fitted model to get predictions for test set","metadata":{}},{"cell_type":"code","source":"model_preds = model.predict(x_test)\nmodel_preds","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:41.442057Z","start_time":"2022-02-05T15:50:41.414459Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.101019Z","iopub.execute_input":"2022-03-20T11:08:42.101350Z","iopub.status.idle":"2022-03-20T11:08:42.118630Z","shell.execute_reply.started":"2022-03-20T11:08:42.101305Z","shell.execute_reply":"2022-03-20T11:08:42.117779Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* explore model quality on test set.","metadata":{}},{"cell_type":"code","source":"print('model metrics on test set')\nmetrics_test_2 = get_metrics(y_true=y_test, y_pred=model_preds)\nmetrics_test_2","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:41.472214Z","start_time":"2022-02-05T15:50:41.445054Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.120191Z","iopub.execute_input":"2022-03-20T11:08:42.120658Z","iopub.status.idle":"2022-03-20T11:08:42.137555Z","shell.execute_reply.started":"2022-03-20T11:08:42.120615Z","shell.execute_reply":"2022-03-20T11:08:42.136737Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Let's compare `metrics_test_2` against `metrics_test_1`","metadata":{}},{"cell_type":"code","source":"# combine 2 series in 1 dataframe\nmetrics_comparison = metrics_test_1.to_frame('LR_1v').join(metrics_test_2.rename('LR_mv')).T\nmetrics_comparison","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:41.503739Z","start_time":"2022-02-05T15:50:41.475213Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.139253Z","iopub.execute_input":"2022-03-20T11:08:42.139705Z","iopub.status.idle":"2022-03-20T11:08:42.157952Z","shell.execute_reply.started":"2022-03-20T11:08:42.139664Z","shell.execute_reply":"2022-03-20T11:08:42.157104Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14, 5))  # specify figure size\nmetrics_comparison.plot(\n    kind='bar', layout=(1,4), subplots=True, \n    title='change in metrics', legend=False, ax=ax\n);\nfig.tight_layout();  # prettify subplots. read more in documentation if interested","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.53693Z","start_time":"2022-02-05T15:50:41.507803Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.159093Z","iopub.execute_input":"2022-03-20T11:08:42.159516Z","iopub.status.idle":"2022-03-20T11:08:42.807724Z","shell.execute_reply.started":"2022-03-20T11:08:42.159480Z","shell.execute_reply":"2022-03-20T11:08:42.806868Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"* did our Linear regression model improve after we've added more features?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Task 9. Implement Linear regression model with multiple variables yourself (5 points)\n\nOkay, we've fitted Linear regression model using `sklearn` library. Now let's do the math ourselves!","metadata":{}},{"cell_type":"markdown","source":"### Prerequisites\n\nFor this task you will need to:\n* have basic understanding of Linear algebra (operations with vectors and matrices)\n* know how to perform matrix multiplication, transposition and inversion using `numpy` \n  (these functions can be found in NumPy documentation)\n* carefully read the theory below and refer to wiki / textbooks / other sources if required\n\n---\n\n### Theory and notation\n\nWhen we extend Linear regression model on case with $m$ variables we get the following formula:\n\n* $ \\hat{y}\n= 1 \\cdot w_0 + x_1 \\cdot w_1 + x_2 \\cdot w_2 + ... + x_m \\cdot w_m\n= \\sum_{i=0}^{m} x_i w_i\n$,\n\nwhere:\n* $x = (1, x_1, ..., x_m)$ is an _(m+1)_-element feature vector. \n    * $x_0 = 1$ is a special term introduced in the beginning of feature vector \n      to add intercept to Linear regression model\n    * $x_i, i=\\overline{1,m}$ is an _i_-th feature value.\n* $w = (w_0, w_1, ..., w_m)$ is an _(m+1)_-element parameter vector\n    * $w_0$ - the first value of parameter vector - is referred to as **intercept**\n* $\\hat{y}$ is a predicted value\n\n----\n\nThe formula above is used to calculate predictions for 1 sample.\ni.e. we predict 1 scalar $\\hat{y}$ from 1 feature vector $x$.\n\nIf we have a dataset of $n$ samples $(x, y)$ - we get a system of linear equations:\n\n$\n\\begin{cases}\n \\hat{y}^{(0)} = \\sum_{i=0}^{m} x_i^{(0)} w_i \\\\\n \\hat{y}^{(1)} = \\sum_{i=0}^{m} x_i^{(1)} w_i \\\\\n ... \\\\\\\n \\hat{y}^{(n-1)} = \\sum_{i=0}^{m} x_i^{(n-1)} w_i\n\\end{cases}\n$\n\nSuch system can be rewritten in a succinct matrix form (a way that we write equations in Linear algebra):\n\n* $ \\hat{y} = X \\cdot w $\n\nAnd the task of minimizing Residual sum of squares (RSS) now looks:\n* $ RSS(w, x, y) = \\|y - \\hat{y}\\|_2^2 = \\|y - X \\cdot w \\|_2^2 \\rightarrow min $\n\nHere:\n* $y = (y^{(0)}, y^{(1)}, ..., y^{(n-1)})$ now denotes a **vector of target values** we want to predict<br>\n  $y^{(j)}$ is a target value for _j_-th sample in a dataset.\n* $\\hat{y} = (\\hat{y}^{(0)}, ..., \\hat{y}^{(n-1)})$ \n  now denotes a **vector of predictions** <br>\n  $\\hat{y}^{(j)}$ denotes a predicted value for _j_-th sample in a dataset\n* $X$ is a **feature matrix** of shape $n \\times (m+1)$. <br>\n  _n_ rows correspond to _n_ data samples and _m+1_ columns correspond to _m+1_ features that describe each sample.\n* $w = (w_0, w_1, ..., w_m)$ is still a **model parameter vector**\n* $X \\cdot w$ denotes a **matrix-by-vector** multiplication\n* $\\|\\cdot\\|_2$ denotes an $L_2$-norm and is basically a square root of sum of squares of vector elements:<br>\n  $\\|a\\|_2 = \\sqrt{a_0^2 + a_1^2 + ... + a_{n-1}^2}$ for an _n_-component vector _a_.\n  Thus, RSS means the same as before:<br>\n  $ RSS(w, x, y) = \\|y - \\hat{y}\\|_2^2 = \n  (y^{(0)} - \\hat{y}^{(0)})^2 + (y^{(1)} - \\hat{y}^{(1)})^2 + ... + (y^{(n-1)} - \\hat{y}^{(n-1)})^2 = \n  \\sum_{j=0}^{n-1} (y^{(j)} - \\hat{y}^{(j)})^2\n  $\n\nNote:\n* we now use **superscripts** $\\hat{y}^{(i)}$ instead of **subscripts** $\\hat{y}_i$ as before\n  to denote _i_-th sample in a dataset\n* it is done to be able to refer both to sample index (denoted by superscript)\n  and feature index (denoted by subscript) in a single equation. \n  \n  Thus:\n  * $x^{(j)} = (1, x^{(j)}_1, x^{(j)}_2, ..., x^{(j)}_{m})$  now denotes \n    _j_-th feature vector in a dataset with _m+1_ features\n  * $x_i^{(j)}$ denotes an _i_-th feature value of _j_-th feature vector\n  * $X = \n  \\begin{bmatrix}\n      1 & x^{(0)}_1 & x^{(0)}_2 & ... & x^{(0)}_{m} \\\\\n      1 & x^{(1)}_1 & x^{(1)}_2 & ... & x^{(1)}_{m} \\\\\n      ... \\\\\n      1 & x^{(n-1)}_1 & x^{(n-1)}_2 & ... & x^{(n-1)}_{m}\n    \\end{bmatrix}\n    $ is an $n \\times (m+1)$ feature matrix. <br><br>\n    $X$ can also be written as a column-vector of feature vectors:<br><br>\n    $ X = \\begin{bmatrix}x^{(0)} \\\\ x^{(1)} \\\\ ... \\\\ x^{(n-1)}\\end{bmatrix} $\n\n---\n\n### Fit \n\n**<TL;DR\\>** Enough notation! How can we fit this model?\n\nBy minimizin Residual sum of squares (RSS) for a Linear regression equation in a matrix form\nwe now get a **single** formula to find **all model parameters**!\n* $ w = (X^T X)^{-1} X^T y $\n\nBeautiful, isn't it? <br>\nRecall, how we got individual formulas for $w_0, w_1$ in case of Linear regression with 1 predictor.<br>\nNow we can find the whole parameter vector _w_ at once!\n\nIn the formula above:\n* $X$ is an $n \\times (m+1)$ dimensional feature matrix\n\n\n* $X^T$ is a **transposed** feature matrix.<br>\n  Transposition is a mathematical operation of \"swapping\" matrix dimensions.<br>\n  So $X^T$ is an $(m+1) \\times n$ dimensional matrix  \n\n\n* $ (X^T X)^{-1} $ is the inverse matrix of $ X^T X $\n\nNotes:\n* Because of inversion operation performed on $X^T X$ matrix \n  there are difficulties in fitting Linear regression model in some cases.\n  \n  We will talk more on that later in the assignment.\n\n---\n    \nIf you want to learn more you can refer to \n[Linear least squares](https://en.wikipedia.org/wiki/Linear_least_squares)\nwiki-page.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Implementation","metadata":{}},{"cell_type":"markdown","source":"* implement `LRMatrixForm` class and `add_ones_column` function.<br>\n  docstrings describe what each function/method should do - so read the docstrings.\n  \n  \n* [what is a docstring?](https://www.python.org/dev/peps/pep-0257/#what-is-a-docstring)\n\n\nNote:\n* `LRMatrixForm` class must support 2 modes: **with** fitting intercept parameter and **without**.<br>\n  The behavior is controlled by `fit_intercept` parameter passed to class constructor.","metadata":{}},{"cell_type":"code","source":"from sklearn.exceptions import NotFittedError","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.552627Z","start_time":"2022-02-05T15:50:42.543552Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.808873Z","iopub.execute_input":"2022-03-20T11:08:42.809105Z","iopub.status.idle":"2022-03-20T11:08:42.812864Z","shell.execute_reply.started":"2022-03-20T11:08:42.809076Z","shell.execute_reply":"2022-03-20T11:08:42.812122Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"def add_ones_column(x):\n    \"\"\" \n    Add column consisting of ones to the beginning of matrix `x`. \n    Used when fitting `LRMatrixForm` with `fit_intercept` param set to True.\n    \"\"\"\n    # HINTS: \n    # * create a vector that consists of ones: (1, 1, ..., 1)\n    # * then use either `np.column_stack` or `np.hstack` function to add this vector to a matrix\n    \n    # # your code here:\n    # return ...\n    vector = np.ones(x.shape[0])\n    x = np.column_stack((vector,x))\n    return x\n\nclass LRMatrixForm:\n    \"\"\"\n    Class that encapsulates Linear regression model with multiplie variables.\n    Calculations are performed using matrix formula.\n    \n    Use `fit()` method to fit the model on data.\n    Use `predict()` method to calculate predictions when the model is fitted.\n    \"\"\"\n    def __init__(self, fit_intercept: bool = True):\n        \"\"\"\n        fit_intercept: \n        Whether to calculate the intercept for this model. \n        If set to False, no intercept will be used in calculations \n        (i.e. data is expected to be centered).\n        \"\"\"\n        self.fit_intercept = fit_intercept\n        self.w_ = None  # vector with all model parameters (intercept and weights)\n    \n    def fit(self, x_train: pd.DataFrame, y_train: pd.Series):\n        X = x_train.to_numpy()  # convert pd.DataFrame to numpy matrix\n        \n        if self.fit_intercept is True:\n            # when creating a model with intercept term we need to extend feature matrix\n            # with a column consisting of ones (1, 1, ..., 1).\n            X = add_ones_column(X)\n            \n        # # your code here to calculate model parameters:\n        \n        self.w_ = np.dot(np.dot(np.linalg.inv(np.dot(X.transpose(),X)),X.transpose()),y_train)\n        \n    @property\n    def coef_(self):\n        \"\"\" Property that returns model parameters without the intercept term. \"\"\"\n        if self.w_ is None:\n            raise NotFittedError()\n            \n        if self.fit_intercept is True:        # # your code here:\n        # return ...\n        \n            return self.w_[1:]\n        else:\n            return self.w_\n    \n    @property\n    def intercept_(self):\n        \"\"\" Property that returns intercept term from model parameters. \"\"\"\n        if self.fit_intercept is not True:\n            return 0.0  # for compatibility with sklearn\n        if self.w_ is None:\n            raise NotFittedError()\n        # # your code here:\n        # return ...\n        return self.w_[0]\n    \n    def predict(self, x_test: pd.DataFrame):\n        \"\"\" Generate predictions using fitted model parameters. \"\"\"\n        if self.w_ is None:\n            raise NotFittedError()\n        \n        X = x_test.to_numpy()  # convert pd.DataFrame to numpy matrix\n        \n        if self.fit_intercept is True:\n            X = add_ones_column(X)\n            \n        # # your code here to calculate predictions\n        # return ...\n        return np.dot(X,self.w_)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.614912Z","start_time":"2022-02-05T15:50:42.555773Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.814172Z","iopub.execute_input":"2022-03-20T11:08:42.814438Z","iopub.status.idle":"2022-03-20T11:08:42.828607Z","shell.execute_reply.started":"2022-03-20T11:08:42.814408Z","shell.execute_reply":"2022-03-20T11:08:42.827891Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validate your implementation is identical to library version","metadata":{}},{"cell_type":"code","source":"lr_matrix_form = LRMatrixForm(fit_intercept=True)\nfit_and_evaluate(lr_matrix_form, x_train, y_train, x_test, y_test)\n","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.690735Z","start_time":"2022-02-05T15:50:42.649536Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.829808Z","iopub.execute_input":"2022-03-20T11:08:42.830195Z","iopub.status.idle":"2022-03-20T11:08:42.877166Z","shell.execute_reply.started":"2022-03-20T11:08:42.830135Z","shell.execute_reply":"2022-03-20T11:08:42.876354Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"print(f'intercept: {lr_matrix_form.intercept_}')\nprint(f'coef: {lr_matrix_form.coef_}')\n","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.706335Z","start_time":"2022-02-05T15:50:42.69462Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.878812Z","iopub.execute_input":"2022-03-20T11:08:42.879338Z","iopub.status.idle":"2022-03-20T11:08:42.886736Z","shell.execute_reply.started":"2022-03-20T11:08:42.879288Z","shell.execute_reply":"2022-03-20T11:08:42.885960Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"assert np.allclose(model.predict(x_test), lr_matrix_form.predict(x_test))\nassert round(model.intercept_, 9) == round(lr_matrix_form.intercept_, 9)\nassert np.allclose(model.coef_, lr_matrix_form.coef_)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.721626Z","start_time":"2022-02-05T15:50:42.709232Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.888262Z","iopub.execute_input":"2022-03-20T11:08:42.888965Z","iopub.status.idle":"2022-03-20T11:08:42.903023Z","shell.execute_reply.started":"2022-03-20T11:08:42.888888Z","shell.execute_reply":"2022-03-20T11:08:42.902155Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* let's take a look at difference between model predictions and target values:","metadata":{}},{"cell_type":"code","source":"(model.predict(x_test) - y_test).describe()","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.752837Z","start_time":"2022-02-05T15:50:42.724242Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.905089Z","iopub.execute_input":"2022-03-20T11:08:42.905772Z","iopub.status.idle":"2022-03-20T11:08:42.924440Z","shell.execute_reply.started":"2022-03-20T11:08:42.905718Z","shell.execute_reply":"2022-03-20T11:08:42.923408Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now let's compare `sklearn` model and yours with `fit_intercept=False`","metadata":{}},{"cell_type":"code","source":"model = LinearRegression(fit_intercept=False)\nfit_and_evaluate(model, x_train, y_train, x_test, y_test)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.800245Z","start_time":"2022-02-05T15:50:42.756837Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.926234Z","iopub.execute_input":"2022-03-20T11:08:42.926790Z","iopub.status.idle":"2022-03-20T11:08:42.967452Z","shell.execute_reply.started":"2022-03-20T11:08:42.926736Z","shell.execute_reply":"2022-03-20T11:08:42.966540Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"lr_matrix_form = LRMatrixForm(fit_intercept=False)\nfit_and_evaluate(lr_matrix_form, x_train, y_train, x_test, y_test)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.832215Z","start_time":"2022-02-05T15:50:42.803391Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:42.969138Z","iopub.execute_input":"2022-03-20T11:08:42.969749Z","iopub.status.idle":"2022-03-20T11:08:43.005195Z","shell.execute_reply.started":"2022-03-20T11:08:42.969695Z","shell.execute_reply":"2022-03-20T11:08:43.004303Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"* Compare the metrics on a test set of a model fitted **with** intercept parameter \n  vs model fitted **without** intercept.\n* Which model has a better performance? Can you explain why?","metadata":{}},{"cell_type":"code","source":"print(f'intercept: {lr_matrix_form.intercept_}')\nprint(f'coef: {lr_matrix_form.coef_}')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.847828Z","start_time":"2022-02-05T15:50:42.835364Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.006892Z","iopub.execute_input":"2022-03-20T11:08:43.007457Z","iopub.status.idle":"2022-03-20T11:08:43.015051Z","shell.execute_reply.started":"2022-03-20T11:08:43.007403Z","shell.execute_reply":"2022-03-20T11:08:43.014206Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"assert np.allclose(model.predict(x_test), lr_matrix_form.predict(x_test))\nassert round(model.intercept_, 9) == round(lr_matrix_form.intercept_, 9)\nassert np.allclose(model.coef_, lr_matrix_form.coef_)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.863579Z","start_time":"2022-02-05T15:50:42.850971Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.016596Z","iopub.execute_input":"2022-03-20T11:08:43.017085Z","iopub.status.idle":"2022-03-20T11:08:43.030955Z","shell.execute_reply.started":"2022-03-20T11:08:43.017036Z","shell.execute_reply":"2022-03-20T11:08:43.030027Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* let's take a look at difference between model predictions and target values again:","metadata":{}},{"cell_type":"code","source":"(model.predict(x_test) - y_test).describe()","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.894776Z","start_time":"2022-02-05T15:50:42.867045Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.032439Z","iopub.execute_input":"2022-03-20T11:08:43.032901Z","iopub.status.idle":"2022-03-20T11:08:43.051737Z","shell.execute_reply.started":"2022-03-20T11:08:43.032859Z","shell.execute_reply":"2022-03-20T11:08:43.050606Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"* compare this to the same statistics obtained with model fitted **with intercept**\n* notice how the _mean_ value of difference changed","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Limitations of Linear regression\n\nRecall the formula to find Linear regression parameters in case of multiple variables:\n* $ w = (X^T X)^{-1} X^T y $\n\nAs noted above, there might be difficulties with fitting Linear regression model\nbecause of inverting $X^T X$ matrix.\n\nNot every matrix can be inverted.\n\nAn example of such case is when 2 or more features have strong linear relationship between them.<br>\nIt means that one of variables can be **linearly predicted from the others with a substantial degree of accuracy**.<br>\nSuch situation is named **collinearitry** (in case of 2 related variables)\nor **multicollinearity** (>2 related features).<br>\n\nCollinearity is associated with high value of correlation coefficient between 2 related features.\n\nYou can refer to [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) \nwiki page to read more.\n\nThe intuition why multicollinearity is bad is that Linear Regression tries to capture \na contribution of each individual feature to target independently of other features.\nThis is not possible with the data that has collinear features, \nas change in one such variable is always associated with change in other/others.\n\nThere are a whole bunch of really interesting thoughts that can help you \nto capture the intuition behind the concept of multicollinearity.<br>\nYou can find some of them \n[here](https://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r).","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Collinearity\n\n* Let's construct and add a new feature to our dataset that is going to be _collinear_ with another one:","metadata":{}},{"cell_type":"code","source":"# select features as usual\ntarget = 'dribbling'\nfeatures = ['ball_control', 'short_passing', 'strength', 'sprint_speed']\nx_train, y_train, x_test, y_test = get_train_test_data(features, target)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.909912Z","start_time":"2022-02-05T15:50:42.897985Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.053242Z","iopub.execute_input":"2022-03-20T11:08:43.053703Z","iopub.status.idle":"2022-03-20T11:08:43.061517Z","shell.execute_reply.started":"2022-03-20T11:08:43.053660Z","shell.execute_reply":"2022-03-20T11:08:43.060602Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"def add_collinear_column(_df: pd.DataFrame, orig_col: str, new_col: str):    \n    \"\"\" \n    Construct new column as a linear transform of another one + tiny noise.\n    y = f(x) = a1 * x + a2 + noise.\n    \n    Alternatively, we could apply no linear transform at all (set a1=1, a2=0)\n    And only add some noise to existing column. Results would be the same.\n    \"\"\"\n    # linear transform\n    _df[new_col] = 2 * _df[orig_col] + 3\n    # add tiny normal noise\n    _df[new_col] += np.random.normal(loc=0, scale=0.0001, size=_df.shape[0])\n\norig_col = 'sprint_speed'\nnew_col = 'sprint_speed_2'\nadd_collinear_column(x_train, orig_col, new_col)\nadd_collinear_column(x_test, orig_col, new_col)\nfeatures.append(new_col)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.940438Z","start_time":"2022-02-05T15:50:42.913437Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.062987Z","iopub.execute_input":"2022-03-20T11:08:43.063608Z","iopub.status.idle":"2022-03-20T11:08:43.078810Z","shell.execute_reply.started":"2022-03-20T11:08:43.063566Z","shell.execute_reply":"2022-03-20T11:08:43.077724Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"corr_coef = np.corrcoef(x_train[orig_col], x_train[new_col])[0, 1]\nprint(f'correlation between {orig_col} and {new_col}: {corr_coef}')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.956152Z","start_time":"2022-02-05T15:50:42.943901Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.080424Z","iopub.execute_input":"2022-03-20T11:08:43.080912Z","iopub.status.idle":"2022-03-20T11:08:43.088394Z","shell.execute_reply.started":"2022-03-20T11:08:43.080872Z","shell.execute_reply":"2022-03-20T11:08:43.087491Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"* As expected, correlation coefficient between collinear features \n  (`sprint_speed` and `sprint_speed_2`) is close to 1.<br>\n  Thus they are highly correlated.","metadata":{}},{"cell_type":"code","source":"# visualize sample of train data\ncheck = x_train.sample(5).copy()\n# add helper column to see how much noise was added\ncheck['sprint_speed_2_wo_noise'] = 2 * check['sprint_speed'] + 3\ncheck","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:42.987635Z","start_time":"2022-02-05T15:50:42.964593Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.090401Z","iopub.execute_input":"2022-03-20T11:08:43.090982Z","iopub.status.idle":"2022-03-20T11:08:43.113514Z","shell.execute_reply.started":"2022-03-20T11:08:43.090938Z","shell.execute_reply":"2022-03-20T11:08:43.112555Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"### let's fit a Linear Regression model on this feature set:","metadata":{}},{"cell_type":"code","source":"model = LinearRegression(fit_intercept=True)\nfit_and_evaluate(model, x_train, y_train, x_test, y_test)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:43.03481Z","start_time":"2022-02-05T15:50:42.990938Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.115216Z","iopub.execute_input":"2022-03-20T11:08:43.115543Z","iopub.status.idle":"2022-03-20T11:08:43.156711Z","shell.execute_reply.started":"2022-03-20T11:08:43.115500Z","shell.execute_reply":"2022-03-20T11:08:43.155722Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"* Both train and test metrics look reasonable\n* Now let's explore model params:","metadata":{}},{"cell_type":"code","source":"print(f'intercept: {model.intercept_}')\nprint(f'coef: {model.coef_.tolist()}')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:43.050054Z","start_time":"2022-02-05T15:50:43.038334Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.158543Z","iopub.execute_input":"2022-03-20T11:08:43.158856Z","iopub.status.idle":"2022-03-20T11:08:43.164067Z","shell.execute_reply.started":"2022-03-20T11:08:43.158814Z","shell.execute_reply":"2022-03-20T11:08:43.162514Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"* Weights that correspond to `sprint_speed` and `sprint_speed_2` columns should have gone insanely high!\n\n\n* That is not ok. Large weights and weights instability are the sign of \n    [**overfitting**](https://en.wikipedia.org/wiki/Overfitting).\n\n    According to the definition, overfitting is:<br>\n    \"_the production of an analysis that corresponds too closely \n    or exactly to a particular set of data,<br>\n    and may therefore fail to fit additional data \n    or predict future observations reliably_\".\n    \n    \n* But what does that mean? Let's try to generate predictions with our model.","metadata":{}},{"cell_type":"markdown","source":"### generate predictions using our fitted model","metadata":{}},{"cell_type":"code","source":"check = x_test.head(2).copy()\ncheck[target] = y_test.head(2)\ncheck['preds'] = model.predict(check[features])\ncheck","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:43.080517Z","start_time":"2022-02-05T15:50:43.052815Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.165850Z","iopub.execute_input":"2022-03-20T11:08:43.166154Z","iopub.status.idle":"2022-03-20T11:08:43.202019Z","shell.execute_reply.started":"2022-03-20T11:08:43.166115Z","shell.execute_reply":"2022-03-20T11:08:43.201142Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"get_metrics(y_true=check[target], y_pred=check['preds'])","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:43.111095Z","start_time":"2022-02-05T15:50:43.084033Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.203710Z","iopub.execute_input":"2022-03-20T11:08:43.204344Z","iopub.status.idle":"2022-03-20T11:08:43.221088Z","shell.execute_reply.started":"2022-03-20T11:08:43.204298Z","shell.execute_reply":"2022-03-20T11:08:43.220084Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"* predictions look good, right? ","metadata":{"ExecuteTime":{"end_time":"2022-02-04T12:22:55.993894Z","start_time":"2022-02-04T12:22:55.983089Z"}}},{"cell_type":"markdown","source":"### but what if we change `sprint_speed` values a bit? let's add some small `delta`","metadata":{"ExecuteTime":{"end_time":"2022-02-04T12:22:55.993894Z","start_time":"2022-02-04T12:22:55.983089Z"}}},{"cell_type":"code","source":"delta = 1  # some small value\ncheck['sprint_speed'] += delta\ncheck['preds_2'] = model.predict(check[features])\ncheck","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:43.157969Z","start_time":"2022-02-05T15:50:43.114463Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.222984Z","iopub.execute_input":"2022-03-20T11:08:43.223596Z","iopub.status.idle":"2022-03-20T11:08:43.253201Z","shell.execute_reply.started":"2022-03-20T11:08:43.223550Z","shell.execute_reply":"2022-03-20T11:08:43.252220Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"get_metrics(y_true=check[target], y_pred=check['preds_2'])","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:43.190117Z","start_time":"2022-02-05T15:50:43.161045Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.255048Z","iopub.execute_input":"2022-03-20T11:08:43.255669Z","iopub.status.idle":"2022-03-20T11:08:43.269725Z","shell.execute_reply.started":"2022-03-20T11:08:43.255621Z","shell.execute_reply":"2022-03-20T11:08:43.268975Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"* Wow, new predictions doesn't look even close to the target!<br>\n  Our model doesn't look like a good model anymore.\n  \n\n* That is exactly what overfitting is. <br>\n  **We fail to obtain reasonable predictions when we apply our model on new data**.","metadata":{}},{"cell_type":"markdown","source":"### One might notice that we have changed only 1 of 2 collinear features\n* Indeed, giant model weights that correspond to these 2 features, should cancel each one out<br>\n  if relation between feature values stays the same.  ","metadata":{}},{"cell_type":"markdown","source":"* Let's check this out.<br>\n  We have performed following transform:<br>\n  `sprint_speed_changed = sprint_speed + delta`<br>\n  \n\n* How should we change `sprint_speed_2` to match the same data distribution from train data?<br>\n  \n  Let's calculate (we will ignore the noise added):  \n  `sprint_speed_2_changed - sprint_speed_2 = f(sprint_speed_changed) - f(sprint_speed) =`<br>\n  `= a1 * sprint_speed_changed + a2 - (a1 * sprint_speed + a2) = a1 * delta`","metadata":{}},{"cell_type":"code","source":"# here we change the second of collinear features \n# using the same coefficient \n# that was used in linear transform in `add_collinear_column()` function.\n# we don't need to add intercept coefficient as it cancels it out (you can do the math yourself).\ncheck['sprint_speed_2'] += 2 * delta\ncheck['preds_3'] = model.predict(check[features])\ncheck","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:43.236719Z","start_time":"2022-02-05T15:50:43.193445Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.271366Z","iopub.execute_input":"2022-03-20T11:08:43.271674Z","iopub.status.idle":"2022-03-20T11:08:43.290701Z","shell.execute_reply.started":"2022-03-20T11:08:43.271632Z","shell.execute_reply":"2022-03-20T11:08:43.290038Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"* In the last example we see that new predictions (`preds_3`) now look similar to the first ones <br>\n  (`preds` - calculated before changing `sprint_speed` value)\n\n\n* So, if values of collinear features change accordingly - we are safe, right? <br>\n  How do your think, can we always rely on this assumtion?","metadata":{"ExecuteTime":{"end_time":"2022-02-05T09:31:30.2817Z","start_time":"2022-02-05T09:31:30.265131Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What just happened\n\n* The answer to the question above is: **no, we can't**.\n  \n  \n* Yes, we have checked that if collinear features values are changed accordingly \n  (i.e. their **initial relation is preserved**), <br>\n  model predictions look reasonable and accurate.\n  \n  But this will not always be the case on the real data.\n  \n  We can't be sure that the data \n  that we apply our fitted model on (`x_test`, for example)<br>\n  preserves **exactly the same data distribution** as the data that model was trained on (`x_train`).\n  \n  \n\n**Why this happened?**\n\n* When feature matrix contains collinear or multicollinear features (the problem of **multicollinearity**),\n  inversion of such a matrix is numerically unstable.<br>\n  This leads to large values and imprecise estimates of model parameters.\n\n**How to detect multicollinearity**\n* You should monitor your model weights. For example using $L_2$ norm:<br>\n  $ L_2(w) = \\|w\\|_2 = \\sqrt{w_1^2 + w_2^2 + ... + w_n^2} = \\sqrt{\\sum_{i=1}^{n} w_i^2} $<br>\n  If any of model weights are huge - $L_2$ norm will also have large value.\n  \n  \n* Monitor model performance on validation data. \n  If the metrics are poor and prediction look huge,<br>\n  you should check if collinear features are present in train data.\n\n**How to fix the issue**\n\n* For each set of collinear features, keep only 1 of them and remove the rest.<br>\n  The best regression models are those in which the predictor variables each correlate highly \n  with the target variable,<br>\n  but correlate at most only minimally with each other.\n  \n\n* Combine collinear features into new ones\n\n  \n* Use regularization (will be covered later in the course).","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Task 10. Implement function to calculate $L_2$-norm. (1 point)\n* Use formula above to implement $L_2$ norm","metadata":{}},{"cell_type":"code","source":"def l2_norm(arr: Iterable):\n    # your code here\n    \n    return np.sqrt(np.sum(arr**2))\n    #raise NotImplementedError()","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:43.252641Z","start_time":"2022-02-05T15:50:43.239359Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.291868Z","iopub.execute_input":"2022-03-20T11:08:43.292317Z","iopub.status.idle":"2022-03-20T11:08:43.300549Z","shell.execute_reply.started":"2022-03-20T11:08:43.292258Z","shell.execute_reply":"2022-03-20T11:08:43.299771Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculate $L_2$-norm value for current model\n* you should get high value that represents large values of some model parameters","metadata":{}},{"cell_type":"code","source":"l2_norm(model.coef_)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:43.283426Z","start_time":"2022-02-05T15:50:43.271895Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.301673Z","iopub.execute_input":"2022-03-20T11:08:43.302295Z","iopub.status.idle":"2022-03-20T11:08:43.314710Z","shell.execute_reply.started":"2022-03-20T11:08:43.302254Z","shell.execute_reply":"2022-03-20T11:08:43.313794Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multicollinearity\n  \n* It's important to notice that in case of multicollinearity,\n  correlation coefficient between each pair of related variables\n  could be **not so high**.\n  \n  So **it may be hard to find multicollinear features in your data**.\n  \n\n* Let's visualize it in practice.\n\n  We will build a new feature that has a strong linear relationship with 2 other features:\n  $x_1 = a_0 + a_2 x_2 + a_3 x_3 + noise$\n\n  We want to show that Linear regression model will still suffer from multicollinearity,<br>\n  but correlation coefficients betweent each pair of multicollinear features will be **less close to 1**\n  than it was in case of collinearity.","metadata":{}},{"cell_type":"code","source":"target = 'dribbling'\nfeatures = ['ball_control', 'short_passing', 'strength', 'sprint_speed']\nx_train, y_train, x_test, y_test = get_train_test_data(features, target)\n\ndef add_collinear_column(_df: pd.DataFrame, col1: str, col2: str, new_col: str):    \n    _df[new_col] = _df[col1] + _df[col2] + 5\n    _df[new_col] += np.random.normal(loc=0, scale=0.0001, size=_df.shape[0])\n\nadd_collinear_column(x_train, col1='sprint_speed', col2='strength', new_col='foo')\nadd_collinear_column(x_test, col1='sprint_speed', col2='strength', new_col='foo')\nfeatures.append(new_col)\n\nmodel = LinearRegression(fit_intercept=True)\nm = fit_and_evaluate(model, x_train, y_train, x_test, y_test)\ndisplay(m)\nprint(f'intercept: {model.intercept_}')\nprint(f'coef: {model.coef_.tolist()}')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:43.345601Z","start_time":"2022-02-05T15:50:43.287206Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.315568Z","iopub.execute_input":"2022-03-20T11:08:43.316056Z","iopub.status.idle":"2022-03-20T11:08:43.364930Z","shell.execute_reply.started":"2022-03-20T11:08:43.316025Z","shell.execute_reply":"2022-03-20T11:08:43.364106Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"* As before, model parameters for multicollinear features should be large\n\n* Let's calculate $L_2$-norm value - it should also be large","metadata":{}},{"cell_type":"code","source":"l2_norm(model.coef_)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:43.361054Z","start_time":"2022-02-05T15:50:43.348627Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.366305Z","iopub.execute_input":"2022-03-20T11:08:43.367019Z","iopub.status.idle":"2022-03-20T11:08:43.376928Z","shell.execute_reply.started":"2022-03-20T11:08:43.366972Z","shell.execute_reply":"2022-03-20T11:08:43.376063Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Task 11. Visualize correlation matrix for `x_train`  (1 point)\n* Build annotated heatmap (use `annot` param to add values to plot, select diverging `cmap` and center it at 0)\n* You should see that `foo` feature is correlated with ones it was created from,<br>\n  but each of these correlation coefficients **is not so close to 1**","metadata":{}},{"cell_type":"code","source":"# your code here\nsns.heatmap(x_train.corr(), annot =True,cmap='BuPu', center=0)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:43.376553Z","start_time":"2022-02-05T15:50:43.3643Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.379703Z","iopub.execute_input":"2022-03-20T11:08:43.380625Z","iopub.status.idle":"2022-03-20T11:08:43.777090Z","shell.execute_reply.started":"2022-03-20T11:08:43.380579Z","shell.execute_reply":"2022-03-20T11:08:43.776149Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Regularization\n\n* The concept of regularization (that was mentioned above) is simple.\n\n  We modify Loss function (RSS in case of Linear regression) in such a way<br>\n  to penalize for large values of model parameters.\n  \n  \n* $L_2$ regularization, also named as \n  [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization), \n  adds $L_2$ term to loss function:\n\n  $L(w,x,y) = RSS(w,y) + \\alpha L_2(w) = \\|y - \\hat{y}\\|_2^2 + \\alpha \\|w\\|_2^2 \n  = \\|y - X \\cdot w \\|_2^2 + \\alpha \\|w\\|_2^2$\n  \n  parameter $ \\alpha \\ge 0 $ controls the ammount of regularization applied\n  \n\n* Linear regression implemented with $L_2$ regularization is named a **Ridge regression**.\n\n\n* Ridge regression is implemented in `Ridge` class in `sklearn` library\n\n---\n\nLet's see how applying regularization changes our model.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:43.937621Z","start_time":"2022-02-05T15:50:43.926206Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.778092Z","iopub.execute_input":"2022-03-20T11:08:43.778325Z","iopub.status.idle":"2022-03-20T11:08:43.784519Z","shell.execute_reply.started":"2022-03-20T11:08:43.778296Z","shell.execute_reply":"2022-03-20T11:08:43.783729Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"* at first fit `Ridge` without any regularization to ensure we get similar results \n  to using `LinearRegression`","metadata":{}},{"cell_type":"code","source":"# `solver` parameters determines the algorithm used to solve regression task.\n# we fix it to be `svd` to achieve desired results.\nmodel_r = Ridge(alpha=0, fit_intercept=True, solver='svd')\nfit_and_evaluate(model_r, x_train, y_train, x_test, y_test)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:44.001329Z","start_time":"2022-02-05T15:50:43.942367Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.785751Z","iopub.execute_input":"2022-03-20T11:08:43.785955Z","iopub.status.idle":"2022-03-20T11:08:43.831705Z","shell.execute_reply.started":"2022-03-20T11:08:43.785927Z","shell.execute_reply":"2022-03-20T11:08:43.830834Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"print(f'intercept: {model_r.intercept_}')\nprint(f'coef: {model_r.coef_.tolist()}')\nprint(f'l2 norm: {l2_norm(model_r.coef_)}')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:44.016893Z","start_time":"2022-02-05T15:50:44.004264Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.833366Z","iopub.execute_input":"2022-03-20T11:08:43.833799Z","iopub.status.idle":"2022-03-20T11:08:43.847318Z","shell.execute_reply.started":"2022-03-20T11:08:43.833756Z","shell.execute_reply":"2022-03-20T11:08:43.846200Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"* some model parameters should be large, as well as the $L_2$-norm value\n* now, let's add some regularization","metadata":{}},{"cell_type":"code","source":"model_r = Ridge(alpha=0.01, fit_intercept=True, solver='svd')\nfit_and_evaluate(model_r, x_train, y_train, x_test, y_test)","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:44.063416Z","start_time":"2022-02-05T15:50:44.020333Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.851622Z","iopub.execute_input":"2022-03-20T11:08:43.855483Z","iopub.status.idle":"2022-03-20T11:08:43.900640Z","shell.execute_reply.started":"2022-03-20T11:08:43.855424Z","shell.execute_reply":"2022-03-20T11:08:43.899716Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"print(f'intercept: {model_r.intercept_}')\nprint(f'coef: {model_r.coef_.tolist()}')\nprint(f'l2 norm: {l2_norm(model_r.coef_)}')","metadata":{"ExecuteTime":{"end_time":"2022-02-05T15:50:44.07898Z","start_time":"2022-02-05T15:50:44.066065Z"},"execution":{"iopub.status.busy":"2022-03-20T11:08:43.906034Z","iopub.execute_input":"2022-03-20T11:08:43.908635Z","iopub.status.idle":"2022-03-20T11:08:43.920406Z","shell.execute_reply.started":"2022-03-20T11:08:43.908577Z","shell.execute_reply":"2022-03-20T11:08:43.919243Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"* notice how parameter values changed\n* also notice how $L_2$-norm values decreased","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 12. Putting all together. (up to 10 points)\n\nIn the final task we'll try to recap all that we have learnt so far.\n\nYour task is to build a Linear regression model with multiple variables to predict `agility` variable.<br>\nOnly `LinearRegression` class is allowed to be used for modeling.<br>\n\nSteps you need to follow:\n\n* Make a new train/test split with new proportion: 70% on train and 30% on test data **(1 point)**\n\n\n* Fit a model **(up to 6 points)**\n    \n    * Choose >= 10 features from training set as initial set of features\n      Explain your choice. **(1 point)**\n      \n    * Visualize correlation matrix for selected features **(1 point)**\n    \n    * Analyze if collinear/multicollinear features are present in your feature set.<br>\n      Process collinear/multicollinear features if they are present:<br>\n      remove redundant features, combine features into new ones, etc **(up to 3 points)**\n     \n    * Fit the model and calculate metrics on train and test sets<br>\n      **(1 point max. -0.25 points per each metric that was not implemented in the beginning of the assignment)**\n    \n\n* Analyze fitted model performance **(up to 2 points)**:\n    * Print fitted model parameters and calculate their $L_2$-norm value **(1 point)**\n    * Make conclusion whether your model overfitted or not. Provide your arguments. **(1 point)**\n\n\n* Write a short report about the work done **(1 point)**<br>\n  Can you find a logical explanation for high correlation of some of your features?<br>\n  Are you satisfied with the quality of predictions?<br>\n  How could you pottentially improve the model?<br>\n  Any other thoughts.\n\n\n**Penalties:**\n- **0 points are assigned for the whole task** if used a model different from:\n    - `sklearn.linear_model.LinearRegression`\n    - or `LRMatrixForm`","metadata":{}},{"cell_type":"markdown","source":"*Make a new train/test split with new proportion: 70% on train and 30% on test data(1 point)","metadata":{}},{"cell_type":"code","source":"print(df.shape)\nix_split = int(0.7 * df.shape[0])\nDF_TRAIN = df.iloc[:ix_split].copy()\nDF_TEST = df.iloc[ix_split:].copy()\nprint(DF_TRAIN.shape, DF_TEST.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T11:08:43.926521Z","iopub.execute_input":"2022-03-20T11:08:43.929199Z","iopub.status.idle":"2022-03-20T11:08:43.945001Z","shell.execute_reply.started":"2022-03-20T11:08:43.929108Z","shell.execute_reply":"2022-03-20T11:08:43.943928Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"*Fit a model (up to 6 points)","metadata":{}},{"cell_type":"markdown","source":"Choose >= 10 features from training set as initial set of features Explain your choice. (1 point)","metadata":{}},{"cell_type":"code","source":"# Выбрала 15 характеристик с максимальной корреляцией с ловкостью на основании DF_TRAIN\n# \ntarget = 'agility'\nDF_CORR = DF_TRAIN.corr().abs()\nfeatures = DF_CORR.unstack()[target].sort_values(kind=\"quicksort\", ascending =False)[1:11].index.to_list()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-20T11:08:43.950756Z","iopub.execute_input":"2022-03-20T11:08:43.953413Z","iopub.status.idle":"2022-03-20T11:08:44.008841Z","shell.execute_reply.started":"2022-03-20T11:08:43.953355Z","shell.execute_reply":"2022-03-20T11:08:44.007892Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"Visualize correlation matrix for selected features (1 point)","metadata":{}},{"cell_type":"code","source":"#Построим матрицу корреляции характеристик и тепловую диаграмму корреляции\ncols = features + [target]\ncorrelation_matrix = DF_TRAIN[cols].corr()\n\ndisplay(correlation_matrix)\nfig, ax = plt.subplots(figsize=(15,15))\nsns.heatmap(correlation_matrix, annot = True, center = 0, linewidths=.5, ax=ax)\n\n# Много переменных коррелируют между собой\n# Попробуем умешить влияние этих зависимостей на модель","metadata":{"execution":{"iopub.status.busy":"2022-03-20T11:08:44.010128Z","iopub.execute_input":"2022-03-20T11:08:44.010444Z","iopub.status.idle":"2022-03-20T11:08:45.011901Z","shell.execute_reply.started":"2022-03-20T11:08:44.010405Z","shell.execute_reply":"2022-03-20T11:08:45.011013Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"Analyze if collinear/multicollinear features are present in your feature set.\nProcess collinear/multicollinear features if they are present:\nremove redundant features, combine features into new ones, etc (up to 3 points)","metadata":{}},{"cell_type":"code","source":"# Xарактеристика 'dribbling'  и логически предполагает владение другими навыками (ловкость, короткий пасс, контроль мяча)\n# и имеет высокие значениям корреляции с другими характеристиками\n# Считаю возможным ее исключить\n# Xарактеристики 'sprint_speed','acceleration' хорошо коррелиют между собой и каждая из них с 'agility' \n#  Считаю возможным их объединить в 'acc/sprint'\ncols_to_drop = ['dribbling','sprint_speed','acceleration']\nfeatures[:] = [x for x in features if x not in cols_to_drop]\n\nfeatures.append('acc/sprint')\nDF_TRAIN['acc/sprint'] = DF_TRAIN['acceleration']+DF_TRAIN['sprint_speed']\nDF_TEST['acc/sprint'] = DF_TEST['acceleration']+DF_TEST['sprint_speed']\n\nDF_TRAIN.drop(columns=cols_to_drop, inplace=True)\nDF_TEST.drop(columns=cols_to_drop, inplace=True)\n\ncols_process = features + [target]\n\ncorrelation_matrix =DF_TRAIN[cols_process].corr()\ndisplay(correlation_matrix)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-20T11:08:45.013384Z","iopub.execute_input":"2022-03-20T11:08:45.013879Z","iopub.status.idle":"2022-03-20T11:08:45.048851Z","shell.execute_reply.started":"2022-03-20T11:08:45.013833Z","shell.execute_reply":"2022-03-20T11:08:45.048062Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"\nAgility (ловкость) — Данная характеристика, отражает возможность футболиста быстро взаимодействовать (останавливаться/поворачиваться) с мячом, а так же совершать сложные удары, требующие некоторых акробатических навыков (удары с лета). Так же, этот навык влияет на качество исполнения финтов;\n\nBalance (баланс/равновесие) — определяет вероятность удержания на ногах при физическом контакте, столкновениях и отборах мяча соперником. Характеристика важна для игроков обороны, потому что при её низких значениях – возможны неожиданные падения, приводящие к возникновению опасных моментов;\n\nBallControl (Владение) — способность футболиста наиболее эффективно и быстро обрабатывать мяч при приеме. Так же, она влияет на то, как далеко отскочит мяч при первом касании на бегу. В совокупности с «Напором», влияет на вероятность обыгрыша прессингующего соперника в момент приема мяча;\n\nAcceleration (ускорение/разгон) — показатель того, как быстро игрок набирает скорость;\nSprint Speed (Спринтерская скорость/скорость рывков) — показатель максимальной скорости игрока;\nОбъеденила их в acc/sprint, так как смысл набрать максимальную скорость, но очень медлено ее достичь.\n\nStamina (выносливость) — чем выше этот показатель, тем медленнее игрок устает во время матча;\n\nCurve (подкрутка/кривая) — чем выше этот показатель, тем эффективнее игрок может выполнить крученый удар, навес или прострел. Навык эффективен при исполнении крученых ударов с флангов штрафной, с целью переиграть вратаря, а так же при навесах, т.к. защитнику сложнее занять правильную позицию для перехвата, когда мяч сильно подкручен;\n\nCrossing (навесы) — умение, характеризующее исключительно фланговых игроков, которые прорывают оборону на краю поля, а потом делают пас к воротам. Этот навык, помогает сделать более точный навес или прострел с фланга поля;\n\nFinishing (Точность удара/завершение) — навык, определяющий точность любого удара футболиста по воротам с игры и возможность переиграть при этом вратаря и защитников, пытающихся блокировать удар. Так же, этот показатель влияет на умение оказаться в нужном месте и забить гол.\n\nLong Shots (дальние удары) — показатель, характеризующий снайперов, которые бьют из-за пределов штрафной. Футболист с этим развитым навыком, будет бить чаще и точнее;\n\nВсе выбранные характеристики определяют либо влияют на ловкость.","metadata":{}},{"cell_type":"markdown","source":"Fit the model and calculate metrics on train and test sets\n(1 point max. -0.25 points per each metric that was not implemented in the beginning of the assignment)","metadata":{}},{"cell_type":"code","source":"# разделим данные для обучения и тестирования\nx_train, y_train, x_test, y_test = get_train_test_data(features, target)\n\n# модель с использованием библиотеки sklearn.linear_model.LinearRegression\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(x_train, y_train);\n\n# параметры\nprint(f'model_intercept: {model.intercept_}')\nprint(f'model_slope: {model.coef_[0]}')\n\n# предскажем\nmodel_preds = model.predict(x_test)\n\n# метрики\nmetrics_model = get_metrics(y_true=y_test, y_pred=model_preds)\nprint(f'\\nmodel metrics on test set:\\n{metrics_model}')\nprint(f'\\nmodel_l2:{l2_norm(model.coef_)}')\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-20T11:08:45.050131Z","iopub.execute_input":"2022-03-20T11:08:45.050369Z","iopub.status.idle":"2022-03-20T11:08:45.087660Z","shell.execute_reply.started":"2022-03-20T11:08:45.050342Z","shell.execute_reply":"2022-03-20T11:08:45.086688Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"Параметры модели  model_intercept и model_slope не высокие.\nMSE, MAE, MAPE ничего критичного. \nR2 ближе к 1, чем к 0, что хорошо.\nЗначение l2 не высокое, что просто замечательно.\n\nВыглядит хорошо.","metadata":{}},{"cell_type":"code","source":"(model_preds - y_test).describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T11:08:45.090071Z","iopub.execute_input":"2022-03-20T11:08:45.091781Z","iopub.status.idle":"2022-03-20T11:08:45.118923Z","shell.execute_reply.started":"2022-03-20T11:08:45.091727Z","shell.execute_reply":"2022-03-20T11:08:45.118016Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# Проверка модели на переученность\ndef overfitted_checker(df_check,features, target, delta):\n    df_check['preds'] = model.predict(df_check[features])\n    for feature in features:\n        df_check[feature] += delta\n        df_check['preds_2'] = model.predict(df_check[features])\n        error_predict = (df_check['preds']- df_check['preds_2']).max()\n        print(f'Influence at feature {feature}: {error_predict}')\n        print(get_metrics(y_true=df_check[target], y_pred=df_check['preds_2']))\n        \n    return\n# \ncheck = x_test.head(10).copy()\ncheck[target] = y_test.head(10)\n\ndelta = 1  # как в примере выше\n#get_metrics(y_true=check[target], y_pred=check['preds_2'])\noverfitted_checker(check,features, target, delta)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T11:08:45.121154Z","iopub.execute_input":"2022-03-20T11:08:45.122769Z","iopub.status.idle":"2022-03-20T11:08:45.216808Z","shell.execute_reply.started":"2022-03-20T11:08:45.122719Z","shell.execute_reply":"2022-03-20T11:08:45.215832Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"Выглядит хорошо. Каждая из выбранных характеристик хорошо себя повела на внесение дельты.","metadata":{}},{"cell_type":"markdown","source":"Выводы:\nБыла реализована модель линейной регрессии для предсказания характерисики \"ловкости\" на основании 10 характеристик.\n\nВыбор характеристик осуществлялся на основании корреляции с целевым параметром и логиского влияния на характеристику ловкости. \nРегуляция коллинеарности осуществлялась путем удаления из модели характеристики \"владения мячом\" и объедиения 'ускорения' и 'спринтерского бега'.\n\nБыли расчитаны метрики и параметры модели для оценки правильности выбранных характеристик для построения модели.\nНа основании полученных метрик и параметров можно назвать построенную модель имеющей право на жизнь. ","metadata":{}}]}